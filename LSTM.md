
---

### LSTM 的核心组件与数学原理

一个LSTM单元在时间步 `t` 的输入有三个：
1.  **当前输入向量 (Input Vector)**: `x_t` (在我们的模型中，就是 `h_空间(t)`)
2.  **上一个时间步的隐藏状态 (Previous Hidden State)**: `h_{t-1}`
3.  **上一个时间步的细胞状态 (Previous Cell State)**: `C_{t-1}`

它的输出有两个：
1.  **当前时间步的隐藏状态 (Current Hidden State)**: `h_t`
2.  **当前时间步的细胞状态 (Current Cell State)**: `C_t`

这里的 **细胞状态 `C`** 是LSTM的核心，它就像一条传送带，贯穿整个时间序列，负责存储长期记忆。而 **隐藏状态 `h`** 是细胞状态的一个“过滤后”的版本，既作为当前步的输出，也作为控制下一个步的输入之一。

整个计算过程由三个“门”和一个“细胞状态更新”组成，每个“门”都是一个由 **Sigmoid激活函数** 控制的全连接层。Sigmoid函数输出值在0到1之间，非常适合扮演“门”的角色（0代表完全关闭，1代表完全打开）。

---

#### 1. 遗忘门 (Forget Gate, `f_t`)

**目标**: 决定从旧的细胞状态 `C_{t-1}` 中丢弃多少信息。

**公式**:
`f_t = σ(W_f * [h_{t-1}, x_t] + b_f)`

*   **`[h_{t-1}, x_t]`**: 将上一个时间步的输出 `h_{t-1}` 和当前输入 `x_t` 拼接成一个更长的向量。
*   **`W_f`, `b_f`**: 遗忘门专属的、可学习的权重矩阵和偏置项。
*   **`σ` (Sigmoid函数)**: 将计算结果压缩到 `[0, 1]` 区间。输出的 `f_t` 是一个向量，每个元素的值都在0到1之间。
*   **原理**: `f_t` 会与旧的细胞状态 `C_{t-1}` 进行 **逐元素乘法 (element-wise multiplication)**。如果 `f_t` 的某个元素是0，那么 `C_{t-1}` 对应维度的信息就被完全遗忘了；如果是1，就完全保留。

---

#### 2. 输入门 (Input Gate, `i_t`) 与候选细胞状态 (`C̃_t`)

**目标**: 决定哪些新信息 `x_t` 应该被存入细胞状态 `C_t`。这分为两步。

**第一步：决定要更新哪些值。**
**公式**:
`i_t = σ(W_i * [h_{t-1}, x_t] + b_i)`

*   `i_t` 的计算方式和 `f_t` 完全一样，只是用了另一套独立的参数 `W_i`, `b_i`。它同样输出一个 `[0, 1]` 之间的向量，决定了新信息中哪些维度是重要的。

**第二步：创建一个新的候选值向量 `C̃_t`。**
**公式**:
`C̃_t = tanh(W_C * [h_{t-1}, x_t] + b_C)`

*   **`tanh` 函数**: 这是一个双曲正切函数，将输出值压缩到 `[-1, 1]` 区间。它负责生成新的、可能被加入到细胞状态的“候选信息”。
*   `W_C`, `b_C` 是这部分的专属参数。

**原理**: `i_t` 像一个“开关”，决定了候选信息 `C̃_t` 中每一维有多少比例可以被加入到最终的细胞状态中。

---

#### 3. 细胞状态更新 (Cell State Update)

**目标**: 结合“遗忘”和“输入”两步，计算出当前时间步 `t` 的新细胞状态 `C_t`。

**公式**:
`C_t = f_t * C_{t-1} + i_t * C̃_t`

*   **`f_t * C_{t-1}`**: 这部分是 **“遗忘旧信息”**。将旧的细胞状态 `C_{t-1}` 乘以遗忘门 `f_t`，丢掉不再需要的信息。
*   **`i_t * C̃_t`**: 这部分是 **“增加新信息”**。将候选信息 `C̃_t` 乘以输入门 `i_t`，筛选出重要的新信息。
*   **`+`**: 两部分相加，就得到了更新后的、包含了长期记忆的新细胞状态 `C_t`。

---

#### 4. 输出门 (Output Gate, `o_t`)

**目标**: 决定细胞状态 `C_t` 中的哪些信息，应该作为当前时间步的隐藏状态 `h_t` 输出。

**第一步：决定要输出哪些部分。**
**公式**:
`o_t = σ(W_o * [h_{t-1}, x_t] + b_o)`

*   `o_t` 的计算方式也完全一样，使用独立的参数 `W_o`, `b_o`。它决定了细胞状态中哪些部分可以被“看见”。

**第二步：计算最终的隐藏状态 `h_t`。**
**公式**:
`h_t = o_t * tanh(C_t)`

*   **`tanh(C_t)`**: 先将更新后的细胞状态 `C_t` 通过 `tanh` 函数压缩到 `[-1, 1]` 区间，进行数值上的规整。
*   **`o_t *`**: 然后，将规整后的细胞状态乘以输出门 `o_t`。这就像一个“过滤器”或“遮罩”，只让 `o_t` 允许的部分信息通过，作为最终的隐藏状态 `h_t` 输出。

这个 `h_t` 就是我们需要的、在 `t` 时刻对节点状态的总结，它将被用于最终的预测，并作为 `h_{t}` 传递给下一个时间步 `t+1`。

**总结**: LSTM通过 **遗忘门、输入门、输出门** 这三个精巧的、可学习的门控机制，实现了对长期记忆（细胞状态 `C`）的动态、选择性读写。这使得它能够有效地捕捉时间序列数据中的长期依赖关系，避免了传统RNN的梯度问题，是处理时序问题的强大工具。